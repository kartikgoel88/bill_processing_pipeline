# Example configuration for the refactored bill processing pipeline.
# Override with env: INPUT_ROOT, OUTPUT_DIR, LLM_PROVIDER, LLM_BASE_URL, LLM_MODEL, etc.

input_root: test_input
output_dir: test_output
policy_path: test_output/policy_allowances.json
audit_log_path: test_output/audit_trail.log
log_level: INFO
dry_run: false
max_workers: 1

llm:
  provider: ollama          # ollama | openai | huggingface (default for vision & decision)
  # Optional: separate provider/URL for vision or decision (e.g. ollama for vision, openai for decision)
  # vision_provider: ollama
  decision_provider: openai  # use OpenAI for decision when decision_base_url is OpenAI
  base_url: "http://localhost:11434/v1"
  # vision_base_url: "http://vision-host:11434/v1"
  decision_base_url: "https://api.openai.com/v1"
  api_key: ""   # set in .env as LLM_API_KEY or OPENAI_API_KEY when using OpenAI
  model: qwen2.5:7b
  vision_model: qwen2.5vl:latest
  decision_model: gpt-4o-mini
  max_retries: 3
  retry_delay_sec: 2.0
  timeout_sec: 120

ocr:
  engine: tesseract         # tesseract (default) | easyocr

extraction:
  strategy: ocr_only          # ocr_only | fusion | vision_first
  vision_extractor: donut
  vision_backend: ollama
  confidence_threshold: 0.6  # when fusion: only call vision LLM when OCR confidence < this
  fallback_enabled: true
  fallback_threshold: 0.6
